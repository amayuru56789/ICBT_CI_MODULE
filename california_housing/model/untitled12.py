# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cWRYKGTONjEe4oiPFWwIgVVDwjaTPNTi
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
import warnings
warnings.filterwarnings('ignore')

# Load the data
train = pd.read_csv('/content/drive/MyDrive/kaggle_compition/train.csv')
test = pd.read_csv('/content/drive/MyDrive/kaggle_compition/test.csv')

# EDA
print("Train Data Shape:", train.shape)
print("Test Data Shape:", test.shape)
print("\nTrain Data Info:")
print(train.info())
print("\nTrain Data Description:")
print(train.describe())

# Visualize distributions
plt.figure(figsize=(15, 10))
for i, col in enumerate(train.columns[1:-1]):  # Skip id and target
    plt.subplot(3, 3, i+1)
    sns.histplot(train[col], kde=True)
    plt.title(col)
plt.tight_layout()
plt.show()

# Correlation heatmap
plt.figure(figsize=(10, 8))
corr = train.corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', center=0)
plt.title('Feature Correlation')
plt.show()

# Feature engineering
def create_features(df):
    # Create new features
    df['RoomsPerHousehold'] = df['AveRooms'] / df['AveOccup']
    df['BedroomsPerRoom'] = df['AveBedrms'] / df['AveRooms']
    df['PopulationPerHousehold'] = df['Population'] / df['AveOccup']

    # Distance from coast feature
    df['DistanceFromCoast'] = np.sqrt(
        (df['Latitude'] - 34.5)**2 + (df['Longitude'] - (-120))**2
    )
    return df

train = create_features(train)
test = create_features(test)

# Prepare data for modeling
X = train.drop(['id', 'MedHouseVal'], axis=1)
y = train['MedHouseVal']
X_test = test.drop('id', axis=1)

# Train-test split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Model training and evaluation
models = {
    'Random Forest': RandomForestRegressor(n_estimators=200, random_state=42),
    'XGBoost': XGBRegressor(n_estimators=200, learning_rate=0.1, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, random_state=42),
    'SVR': SVR(kernel='rbf', C=10, epsilon=0.1),
    'Neural Network': MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)
}

results = {}
for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_val_scaled)
    rmse = np.sqrt(mean_squared_error(y_val, y_pred))
    results[name] = rmse
    print(f"{name} RMSE: {rmse:.4f}")

# Select best model
best_model_name = min(results, key=results.get)
best_model = models[best_model_name]
print(f"\nBest Model: {best_model_name} with RMSE: {results[best_model_name]:.4f}")

# Feature importance for tree-based models
if hasattr(best_model, 'feature_importances_'):
    feature_importance = pd.DataFrame({
        'Feature': X.columns,
        'Importance': best_model.feature_importances_
    }).sort_values('Importance', ascending=False)

    plt.figure(figsize=(10, 6))
    sns.barplot(x='Importance', y='Feature', data=feature_importance)
    plt.title('Feature Importance')
    plt.show()

# Final prediction on test set
best_model.fit(X_train_scaled, y_train)  # Retrain on full training data
test_pred = best_model.predict(X_test_scaled)

from sklearn.model_selection import GridSearchCV
from xgboost import XGBRegressor
params = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15],
    'learning_rate': [0.01, 0.1, 0.2]
}
grid = GridSearchCV(XGBRegressor(), params, cv=5)
grid.fit(X_train, y_train)

# After model training (continuing from previous code)

# 1. Display sample predictions vs actual values
val_predictions = best_model.predict(X_val_scaled)
results_df = pd.DataFrame({
    'Actual': y_val.values,
    'Predicted': val_predictions,
    'Difference': np.abs(y_val.values - val_predictions)
})

print("\nSample Validation Predictions:")
print(results_df.head(10))

# 2. Calculate and display key metrics
from sklearn.metrics import mean_absolute_error, r2_score

rmse = np.sqrt(mean_squared_error(y_val, val_predictions))
mae = mean_absolute_error(y_val, val_predictions)
r2 = r2_score(y_val, val_predictions)

print("\nModel Evaluation Metrics:")
print(f"RMSE: {rmse:.4f}")
print(f"MAE: {mae:.4f}")
print(f"RÂ² Score: {r2:.4f}")

# 3. Visualize predictions vs actual
plt.figure(figsize=(10, 6))
plt.scatter(y_val, val_predictions, alpha=0.3)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values')
plt.show()

# 4. Residual plot
residuals = y_val - val_predictions
plt.figure(figsize=(10, 6))
sns.histplot(residuals, kde=True)
plt.title('Distribution of Residuals')
plt.xlabel('Prediction Error')
plt.show()

# 5. Feature importance visualization (if available)
if hasattr(best_model, 'feature_importances_'):
    importances = best_model.feature_importances_
    indices = np.argsort(importances)[::-1]

    plt.figure(figsize=(12, 6))
    plt.title("Feature Importances")
    plt.bar(range(X_train.shape[1]), importances[indices], align="center")
    plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)
    plt.tight_layout()
    plt.show()

# 6. Show test predictions
test_predictions = best_model.predict(X_test_scaled)
print("\nSample Test Predictions:")
print(pd.DataFrame(test_predictions, columns=['Predicted_MedHouseVal']).head(10))

# Prepare submission
submission = pd.DataFrame({
    'id': test['id'],
    'MedHouseVal': test_pred
})
submission.to_csv('submission.csv', index=False)
print("\nSubmission file created: submission.csv")